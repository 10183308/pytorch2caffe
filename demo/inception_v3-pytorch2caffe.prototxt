name: "pytorch"
input: "data"
input_dim: 1
input_dim: 3
input_dim: 299
input_dim: 299

layer {
    name: "ConvNdBackward1"
    type: "Convolution"
    bottom: "data"
    top: "ConvNdBackward1"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 2
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward2_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward1"
    top: "BatchNormBackward2"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward2_scale"
    type: "Scale"
    bottom: "BatchNormBackward2"
    top: "BatchNormBackward2"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward3"
    type: "ReLU"
    bottom: "BatchNormBackward2"
    top: "BatchNormBackward2"
}
layer {
    name: "ConvNdBackward4"
    type: "Convolution"
    bottom: "BatchNormBackward2"
    top: "ConvNdBackward4"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward5_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward4"
    top: "BatchNormBackward5"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward5_scale"
    type: "Scale"
    bottom: "BatchNormBackward5"
    top: "BatchNormBackward5"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward6"
    type: "ReLU"
    bottom: "BatchNormBackward5"
    top: "BatchNormBackward5"
}
layer {
    name: "ConvNdBackward7"
    type: "Convolution"
    bottom: "BatchNormBackward5"
    top: "ConvNdBackward7"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward8_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward7"
    top: "BatchNormBackward8"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward8_scale"
    type: "Scale"
    bottom: "BatchNormBackward8"
    top: "BatchNormBackward8"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward9"
    type: "ReLU"
    bottom: "BatchNormBackward8"
    top: "BatchNormBackward8"
}
layer {
    name: "MaxPool2dBackward10"
    type: "Pooling"
    bottom: "BatchNormBackward8"
    top: "MaxPool2dBackward10"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConvNdBackward11"
    type: "Convolution"
    bottom: "MaxPool2dBackward10"
    top: "ConvNdBackward11"
    convolution_param {
        num_output: 80
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward12_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward11"
    top: "BatchNormBackward12"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward12_scale"
    type: "Scale"
    bottom: "BatchNormBackward12"
    top: "BatchNormBackward12"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward13"
    type: "ReLU"
    bottom: "BatchNormBackward12"
    top: "BatchNormBackward12"
}
layer {
    name: "ConvNdBackward14"
    type: "Convolution"
    bottom: "BatchNormBackward12"
    top: "ConvNdBackward14"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward15_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward14"
    top: "BatchNormBackward15"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward15_scale"
    type: "Scale"
    bottom: "BatchNormBackward15"
    top: "BatchNormBackward15"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward16"
    type: "ReLU"
    bottom: "BatchNormBackward15"
    top: "BatchNormBackward15"
}
layer {
    name: "MaxPool2dBackward17"
    type: "Pooling"
    bottom: "BatchNormBackward15"
    top: "MaxPool2dBackward17"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConvNdBackward18"
    type: "Convolution"
    bottom: "MaxPool2dBackward17"
    top: "ConvNdBackward18"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward19_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward18"
    top: "BatchNormBackward19"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward19_scale"
    type: "Scale"
    bottom: "BatchNormBackward19"
    top: "BatchNormBackward19"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward20"
    type: "ReLU"
    bottom: "BatchNormBackward19"
    top: "BatchNormBackward19"
}
layer {
    name: "ConvNdBackward22"
    type: "Convolution"
    bottom: "MaxPool2dBackward17"
    top: "ConvNdBackward22"
    convolution_param {
        num_output: 48
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward23_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward22"
    top: "BatchNormBackward23"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward23_scale"
    type: "Scale"
    bottom: "BatchNormBackward23"
    top: "BatchNormBackward23"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward24"
    type: "ReLU"
    bottom: "BatchNormBackward23"
    top: "BatchNormBackward23"
}
layer {
    name: "ConvNdBackward25"
    type: "Convolution"
    bottom: "BatchNormBackward23"
    top: "ConvNdBackward25"
    convolution_param {
        num_output: 64
        pad_h: 2
        pad_w: 2
        kernel_h: 5
        kernel_w: 5
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward26_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward25"
    top: "BatchNormBackward26"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward26_scale"
    type: "Scale"
    bottom: "BatchNormBackward26"
    top: "BatchNormBackward26"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward27"
    type: "ReLU"
    bottom: "BatchNormBackward26"
    top: "BatchNormBackward26"
}
layer {
    name: "ConvNdBackward29"
    type: "Convolution"
    bottom: "MaxPool2dBackward17"
    top: "ConvNdBackward29"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward30_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward29"
    top: "BatchNormBackward30"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward30_scale"
    type: "Scale"
    bottom: "BatchNormBackward30"
    top: "BatchNormBackward30"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward31"
    type: "ReLU"
    bottom: "BatchNormBackward30"
    top: "BatchNormBackward30"
}
layer {
    name: "ConvNdBackward32"
    type: "Convolution"
    bottom: "BatchNormBackward30"
    top: "ConvNdBackward32"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward33_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward32"
    top: "BatchNormBackward33"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward33_scale"
    type: "Scale"
    bottom: "BatchNormBackward33"
    top: "BatchNormBackward33"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward34"
    type: "ReLU"
    bottom: "BatchNormBackward33"
    top: "BatchNormBackward33"
}
layer {
    name: "ConvNdBackward35"
    type: "Convolution"
    bottom: "BatchNormBackward33"
    top: "ConvNdBackward35"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward36_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward35"
    top: "BatchNormBackward36"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward36_scale"
    type: "Scale"
    bottom: "BatchNormBackward36"
    top: "BatchNormBackward36"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward37"
    type: "ReLU"
    bottom: "BatchNormBackward36"
    top: "BatchNormBackward36"
}
layer {
    name: "AvgPool2dBackward39"
    type: "Pooling"
    bottom: "MaxPool2dBackward17"
    top: "AvgPool2dBackward39"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward40"
    type: "Convolution"
    bottom: "AvgPool2dBackward39"
    top: "ConvNdBackward40"
    convolution_param {
        num_output: 32
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward41_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward40"
    top: "BatchNormBackward41"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward41_scale"
    type: "Scale"
    bottom: "BatchNormBackward41"
    top: "BatchNormBackward41"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward42"
    type: "ReLU"
    bottom: "BatchNormBackward41"
    top: "BatchNormBackward41"
}
layer {
    name: "ConcatBackward43"
    type: "Concat"
    bottom: "BatchNormBackward19"
    bottom: "BatchNormBackward26"
    bottom: "BatchNormBackward36"
    bottom: "BatchNormBackward41"
    top: "ConcatBackward43"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward44"
    type: "Convolution"
    bottom: "ConcatBackward43"
    top: "ConvNdBackward44"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward45_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward44"
    top: "BatchNormBackward45"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward45_scale"
    type: "Scale"
    bottom: "BatchNormBackward45"
    top: "BatchNormBackward45"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward46"
    type: "ReLU"
    bottom: "BatchNormBackward45"
    top: "BatchNormBackward45"
}
layer {
    name: "ConvNdBackward48"
    type: "Convolution"
    bottom: "ConcatBackward43"
    top: "ConvNdBackward48"
    convolution_param {
        num_output: 48
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward49_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward48"
    top: "BatchNormBackward49"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward49_scale"
    type: "Scale"
    bottom: "BatchNormBackward49"
    top: "BatchNormBackward49"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward50"
    type: "ReLU"
    bottom: "BatchNormBackward49"
    top: "BatchNormBackward49"
}
layer {
    name: "ConvNdBackward51"
    type: "Convolution"
    bottom: "BatchNormBackward49"
    top: "ConvNdBackward51"
    convolution_param {
        num_output: 64
        pad_h: 2
        pad_w: 2
        kernel_h: 5
        kernel_w: 5
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward52_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward51"
    top: "BatchNormBackward52"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward52_scale"
    type: "Scale"
    bottom: "BatchNormBackward52"
    top: "BatchNormBackward52"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward53"
    type: "ReLU"
    bottom: "BatchNormBackward52"
    top: "BatchNormBackward52"
}
layer {
    name: "ConvNdBackward55"
    type: "Convolution"
    bottom: "ConcatBackward43"
    top: "ConvNdBackward55"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward56_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward55"
    top: "BatchNormBackward56"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward56_scale"
    type: "Scale"
    bottom: "BatchNormBackward56"
    top: "BatchNormBackward56"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward57"
    type: "ReLU"
    bottom: "BatchNormBackward56"
    top: "BatchNormBackward56"
}
layer {
    name: "ConvNdBackward58"
    type: "Convolution"
    bottom: "BatchNormBackward56"
    top: "ConvNdBackward58"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward59_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward58"
    top: "BatchNormBackward59"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward59_scale"
    type: "Scale"
    bottom: "BatchNormBackward59"
    top: "BatchNormBackward59"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward60"
    type: "ReLU"
    bottom: "BatchNormBackward59"
    top: "BatchNormBackward59"
}
layer {
    name: "ConvNdBackward61"
    type: "Convolution"
    bottom: "BatchNormBackward59"
    top: "ConvNdBackward61"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward62_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward61"
    top: "BatchNormBackward62"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward62_scale"
    type: "Scale"
    bottom: "BatchNormBackward62"
    top: "BatchNormBackward62"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward63"
    type: "ReLU"
    bottom: "BatchNormBackward62"
    top: "BatchNormBackward62"
}
layer {
    name: "AvgPool2dBackward65"
    type: "Pooling"
    bottom: "ConcatBackward43"
    top: "AvgPool2dBackward65"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward66"
    type: "Convolution"
    bottom: "AvgPool2dBackward65"
    top: "ConvNdBackward66"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward67_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward66"
    top: "BatchNormBackward67"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward67_scale"
    type: "Scale"
    bottom: "BatchNormBackward67"
    top: "BatchNormBackward67"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward68"
    type: "ReLU"
    bottom: "BatchNormBackward67"
    top: "BatchNormBackward67"
}
layer {
    name: "ConcatBackward69"
    type: "Concat"
    bottom: "BatchNormBackward45"
    bottom: "BatchNormBackward52"
    bottom: "BatchNormBackward62"
    bottom: "BatchNormBackward67"
    top: "ConcatBackward69"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward70"
    type: "Convolution"
    bottom: "ConcatBackward69"
    top: "ConvNdBackward70"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward71_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward70"
    top: "BatchNormBackward71"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward71_scale"
    type: "Scale"
    bottom: "BatchNormBackward71"
    top: "BatchNormBackward71"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward72"
    type: "ReLU"
    bottom: "BatchNormBackward71"
    top: "BatchNormBackward71"
}
layer {
    name: "ConvNdBackward74"
    type: "Convolution"
    bottom: "ConcatBackward69"
    top: "ConvNdBackward74"
    convolution_param {
        num_output: 48
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward75_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward74"
    top: "BatchNormBackward75"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward75_scale"
    type: "Scale"
    bottom: "BatchNormBackward75"
    top: "BatchNormBackward75"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward76"
    type: "ReLU"
    bottom: "BatchNormBackward75"
    top: "BatchNormBackward75"
}
layer {
    name: "ConvNdBackward77"
    type: "Convolution"
    bottom: "BatchNormBackward75"
    top: "ConvNdBackward77"
    convolution_param {
        num_output: 64
        pad_h: 2
        pad_w: 2
        kernel_h: 5
        kernel_w: 5
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward78_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward77"
    top: "BatchNormBackward78"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward78_scale"
    type: "Scale"
    bottom: "BatchNormBackward78"
    top: "BatchNormBackward78"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward79"
    type: "ReLU"
    bottom: "BatchNormBackward78"
    top: "BatchNormBackward78"
}
layer {
    name: "ConvNdBackward81"
    type: "Convolution"
    bottom: "ConcatBackward69"
    top: "ConvNdBackward81"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward82_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward81"
    top: "BatchNormBackward82"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward82_scale"
    type: "Scale"
    bottom: "BatchNormBackward82"
    top: "BatchNormBackward82"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward83"
    type: "ReLU"
    bottom: "BatchNormBackward82"
    top: "BatchNormBackward82"
}
layer {
    name: "ConvNdBackward84"
    type: "Convolution"
    bottom: "BatchNormBackward82"
    top: "ConvNdBackward84"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward85_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward84"
    top: "BatchNormBackward85"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward85_scale"
    type: "Scale"
    bottom: "BatchNormBackward85"
    top: "BatchNormBackward85"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward86"
    type: "ReLU"
    bottom: "BatchNormBackward85"
    top: "BatchNormBackward85"
}
layer {
    name: "ConvNdBackward87"
    type: "Convolution"
    bottom: "BatchNormBackward85"
    top: "ConvNdBackward87"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward88_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward87"
    top: "BatchNormBackward88"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward88_scale"
    type: "Scale"
    bottom: "BatchNormBackward88"
    top: "BatchNormBackward88"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward89"
    type: "ReLU"
    bottom: "BatchNormBackward88"
    top: "BatchNormBackward88"
}
layer {
    name: "AvgPool2dBackward91"
    type: "Pooling"
    bottom: "ConcatBackward69"
    top: "AvgPool2dBackward91"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward92"
    type: "Convolution"
    bottom: "AvgPool2dBackward91"
    top: "ConvNdBackward92"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward93_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward92"
    top: "BatchNormBackward93"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward93_scale"
    type: "Scale"
    bottom: "BatchNormBackward93"
    top: "BatchNormBackward93"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward94"
    type: "ReLU"
    bottom: "BatchNormBackward93"
    top: "BatchNormBackward93"
}
layer {
    name: "ConcatBackward95"
    type: "Concat"
    bottom: "BatchNormBackward71"
    bottom: "BatchNormBackward78"
    bottom: "BatchNormBackward88"
    bottom: "BatchNormBackward93"
    top: "ConcatBackward95"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward96"
    type: "Convolution"
    bottom: "ConcatBackward95"
    top: "ConvNdBackward96"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 2
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward97_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward96"
    top: "BatchNormBackward97"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward97_scale"
    type: "Scale"
    bottom: "BatchNormBackward97"
    top: "BatchNormBackward97"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward98"
    type: "ReLU"
    bottom: "BatchNormBackward97"
    top: "BatchNormBackward97"
}
layer {
    name: "ConvNdBackward100"
    type: "Convolution"
    bottom: "ConcatBackward95"
    top: "ConvNdBackward100"
    convolution_param {
        num_output: 64
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward101_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward100"
    top: "BatchNormBackward101"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward101_scale"
    type: "Scale"
    bottom: "BatchNormBackward101"
    top: "BatchNormBackward101"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward102"
    type: "ReLU"
    bottom: "BatchNormBackward101"
    top: "BatchNormBackward101"
}
layer {
    name: "ConvNdBackward103"
    type: "Convolution"
    bottom: "BatchNormBackward101"
    top: "ConvNdBackward103"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward104_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward103"
    top: "BatchNormBackward104"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward104_scale"
    type: "Scale"
    bottom: "BatchNormBackward104"
    top: "BatchNormBackward104"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward105"
    type: "ReLU"
    bottom: "BatchNormBackward104"
    top: "BatchNormBackward104"
}
layer {
    name: "ConvNdBackward106"
    type: "Convolution"
    bottom: "BatchNormBackward104"
    top: "ConvNdBackward106"
    convolution_param {
        num_output: 96
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 2
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward107_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward106"
    top: "BatchNormBackward107"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward107_scale"
    type: "Scale"
    bottom: "BatchNormBackward107"
    top: "BatchNormBackward107"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward108"
    type: "ReLU"
    bottom: "BatchNormBackward107"
    top: "BatchNormBackward107"
}
layer {
    name: "MaxPool2dBackward110"
    type: "Pooling"
    bottom: "ConcatBackward95"
    top: "MaxPool2dBackward110"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConcatBackward111"
    type: "Concat"
    bottom: "BatchNormBackward97"
    bottom: "BatchNormBackward107"
    bottom: "MaxPool2dBackward110"
    top: "ConcatBackward111"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward112"
    type: "Convolution"
    bottom: "ConcatBackward111"
    top: "ConvNdBackward112"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward113_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward112"
    top: "BatchNormBackward113"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward113_scale"
    type: "Scale"
    bottom: "BatchNormBackward113"
    top: "BatchNormBackward113"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward114"
    type: "ReLU"
    bottom: "BatchNormBackward113"
    top: "BatchNormBackward113"
}
layer {
    name: "ConvNdBackward116"
    type: "Convolution"
    bottom: "ConcatBackward111"
    top: "ConvNdBackward116"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward117_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward116"
    top: "BatchNormBackward117"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward117_scale"
    type: "Scale"
    bottom: "BatchNormBackward117"
    top: "BatchNormBackward117"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward118"
    type: "ReLU"
    bottom: "BatchNormBackward117"
    top: "BatchNormBackward117"
}
layer {
    name: "ConvNdBackward119"
    type: "Convolution"
    bottom: "BatchNormBackward117"
    top: "ConvNdBackward119"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward120_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward119"
    top: "BatchNormBackward120"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward120_scale"
    type: "Scale"
    bottom: "BatchNormBackward120"
    top: "BatchNormBackward120"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward121"
    type: "ReLU"
    bottom: "BatchNormBackward120"
    top: "BatchNormBackward120"
}
layer {
    name: "ConvNdBackward122"
    type: "Convolution"
    bottom: "BatchNormBackward120"
    top: "ConvNdBackward122"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward123_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward122"
    top: "BatchNormBackward123"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward123_scale"
    type: "Scale"
    bottom: "BatchNormBackward123"
    top: "BatchNormBackward123"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward124"
    type: "ReLU"
    bottom: "BatchNormBackward123"
    top: "BatchNormBackward123"
}
layer {
    name: "ConvNdBackward126"
    type: "Convolution"
    bottom: "ConcatBackward111"
    top: "ConvNdBackward126"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward127_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward126"
    top: "BatchNormBackward127"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward127_scale"
    type: "Scale"
    bottom: "BatchNormBackward127"
    top: "BatchNormBackward127"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward128"
    type: "ReLU"
    bottom: "BatchNormBackward127"
    top: "BatchNormBackward127"
}
layer {
    name: "ConvNdBackward129"
    type: "Convolution"
    bottom: "BatchNormBackward127"
    top: "ConvNdBackward129"
    convolution_param {
        num_output: 128
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward130_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward129"
    top: "BatchNormBackward130"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward130_scale"
    type: "Scale"
    bottom: "BatchNormBackward130"
    top: "BatchNormBackward130"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward131"
    type: "ReLU"
    bottom: "BatchNormBackward130"
    top: "BatchNormBackward130"
}
layer {
    name: "ConvNdBackward132"
    type: "Convolution"
    bottom: "BatchNormBackward130"
    top: "ConvNdBackward132"
    convolution_param {
        num_output: 128
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward133_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward132"
    top: "BatchNormBackward133"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward133_scale"
    type: "Scale"
    bottom: "BatchNormBackward133"
    top: "BatchNormBackward133"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward134"
    type: "ReLU"
    bottom: "BatchNormBackward133"
    top: "BatchNormBackward133"
}
layer {
    name: "ConvNdBackward135"
    type: "Convolution"
    bottom: "BatchNormBackward133"
    top: "ConvNdBackward135"
    convolution_param {
        num_output: 128
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward136_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward135"
    top: "BatchNormBackward136"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward136_scale"
    type: "Scale"
    bottom: "BatchNormBackward136"
    top: "BatchNormBackward136"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward137"
    type: "ReLU"
    bottom: "BatchNormBackward136"
    top: "BatchNormBackward136"
}
layer {
    name: "ConvNdBackward138"
    type: "Convolution"
    bottom: "BatchNormBackward136"
    top: "ConvNdBackward138"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward139_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward138"
    top: "BatchNormBackward139"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward139_scale"
    type: "Scale"
    bottom: "BatchNormBackward139"
    top: "BatchNormBackward139"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward140"
    type: "ReLU"
    bottom: "BatchNormBackward139"
    top: "BatchNormBackward139"
}
layer {
    name: "AvgPool2dBackward142"
    type: "Pooling"
    bottom: "ConcatBackward111"
    top: "AvgPool2dBackward142"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward143"
    type: "Convolution"
    bottom: "AvgPool2dBackward142"
    top: "ConvNdBackward143"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward144_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward143"
    top: "BatchNormBackward144"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward144_scale"
    type: "Scale"
    bottom: "BatchNormBackward144"
    top: "BatchNormBackward144"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward145"
    type: "ReLU"
    bottom: "BatchNormBackward144"
    top: "BatchNormBackward144"
}
layer {
    name: "ConcatBackward146"
    type: "Concat"
    bottom: "BatchNormBackward113"
    bottom: "BatchNormBackward123"
    bottom: "BatchNormBackward139"
    bottom: "BatchNormBackward144"
    top: "ConcatBackward146"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward147"
    type: "Convolution"
    bottom: "ConcatBackward146"
    top: "ConvNdBackward147"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward148_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward147"
    top: "BatchNormBackward148"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward148_scale"
    type: "Scale"
    bottom: "BatchNormBackward148"
    top: "BatchNormBackward148"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward149"
    type: "ReLU"
    bottom: "BatchNormBackward148"
    top: "BatchNormBackward148"
}
layer {
    name: "ConvNdBackward151"
    type: "Convolution"
    bottom: "ConcatBackward146"
    top: "ConvNdBackward151"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward152_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward151"
    top: "BatchNormBackward152"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward152_scale"
    type: "Scale"
    bottom: "BatchNormBackward152"
    top: "BatchNormBackward152"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward153"
    type: "ReLU"
    bottom: "BatchNormBackward152"
    top: "BatchNormBackward152"
}
layer {
    name: "ConvNdBackward154"
    type: "Convolution"
    bottom: "BatchNormBackward152"
    top: "ConvNdBackward154"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward155_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward154"
    top: "BatchNormBackward155"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward155_scale"
    type: "Scale"
    bottom: "BatchNormBackward155"
    top: "BatchNormBackward155"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward156"
    type: "ReLU"
    bottom: "BatchNormBackward155"
    top: "BatchNormBackward155"
}
layer {
    name: "ConvNdBackward157"
    type: "Convolution"
    bottom: "BatchNormBackward155"
    top: "ConvNdBackward157"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward158_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward157"
    top: "BatchNormBackward158"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward158_scale"
    type: "Scale"
    bottom: "BatchNormBackward158"
    top: "BatchNormBackward158"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward159"
    type: "ReLU"
    bottom: "BatchNormBackward158"
    top: "BatchNormBackward158"
}
layer {
    name: "ConvNdBackward161"
    type: "Convolution"
    bottom: "ConcatBackward146"
    top: "ConvNdBackward161"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward162_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward161"
    top: "BatchNormBackward162"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward162_scale"
    type: "Scale"
    bottom: "BatchNormBackward162"
    top: "BatchNormBackward162"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward163"
    type: "ReLU"
    bottom: "BatchNormBackward162"
    top: "BatchNormBackward162"
}
layer {
    name: "ConvNdBackward164"
    type: "Convolution"
    bottom: "BatchNormBackward162"
    top: "ConvNdBackward164"
    convolution_param {
        num_output: 160
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward165_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward164"
    top: "BatchNormBackward165"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward165_scale"
    type: "Scale"
    bottom: "BatchNormBackward165"
    top: "BatchNormBackward165"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward166"
    type: "ReLU"
    bottom: "BatchNormBackward165"
    top: "BatchNormBackward165"
}
layer {
    name: "ConvNdBackward167"
    type: "Convolution"
    bottom: "BatchNormBackward165"
    top: "ConvNdBackward167"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward168_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward167"
    top: "BatchNormBackward168"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward168_scale"
    type: "Scale"
    bottom: "BatchNormBackward168"
    top: "BatchNormBackward168"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward169"
    type: "ReLU"
    bottom: "BatchNormBackward168"
    top: "BatchNormBackward168"
}
layer {
    name: "ConvNdBackward170"
    type: "Convolution"
    bottom: "BatchNormBackward168"
    top: "ConvNdBackward170"
    convolution_param {
        num_output: 160
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward171_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward170"
    top: "BatchNormBackward171"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward171_scale"
    type: "Scale"
    bottom: "BatchNormBackward171"
    top: "BatchNormBackward171"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward172"
    type: "ReLU"
    bottom: "BatchNormBackward171"
    top: "BatchNormBackward171"
}
layer {
    name: "ConvNdBackward173"
    type: "Convolution"
    bottom: "BatchNormBackward171"
    top: "ConvNdBackward173"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward174_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward173"
    top: "BatchNormBackward174"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward174_scale"
    type: "Scale"
    bottom: "BatchNormBackward174"
    top: "BatchNormBackward174"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward175"
    type: "ReLU"
    bottom: "BatchNormBackward174"
    top: "BatchNormBackward174"
}
layer {
    name: "AvgPool2dBackward177"
    type: "Pooling"
    bottom: "ConcatBackward146"
    top: "AvgPool2dBackward177"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward178"
    type: "Convolution"
    bottom: "AvgPool2dBackward177"
    top: "ConvNdBackward178"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward179_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward178"
    top: "BatchNormBackward179"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward179_scale"
    type: "Scale"
    bottom: "BatchNormBackward179"
    top: "BatchNormBackward179"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward180"
    type: "ReLU"
    bottom: "BatchNormBackward179"
    top: "BatchNormBackward179"
}
layer {
    name: "ConcatBackward181"
    type: "Concat"
    bottom: "BatchNormBackward148"
    bottom: "BatchNormBackward158"
    bottom: "BatchNormBackward174"
    bottom: "BatchNormBackward179"
    top: "ConcatBackward181"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward182"
    type: "Convolution"
    bottom: "ConcatBackward181"
    top: "ConvNdBackward182"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward183_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward182"
    top: "BatchNormBackward183"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward183_scale"
    type: "Scale"
    bottom: "BatchNormBackward183"
    top: "BatchNormBackward183"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward184"
    type: "ReLU"
    bottom: "BatchNormBackward183"
    top: "BatchNormBackward183"
}
layer {
    name: "ConvNdBackward186"
    type: "Convolution"
    bottom: "ConcatBackward181"
    top: "ConvNdBackward186"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward187_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward186"
    top: "BatchNormBackward187"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward187_scale"
    type: "Scale"
    bottom: "BatchNormBackward187"
    top: "BatchNormBackward187"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward188"
    type: "ReLU"
    bottom: "BatchNormBackward187"
    top: "BatchNormBackward187"
}
layer {
    name: "ConvNdBackward189"
    type: "Convolution"
    bottom: "BatchNormBackward187"
    top: "ConvNdBackward189"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward190_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward189"
    top: "BatchNormBackward190"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward190_scale"
    type: "Scale"
    bottom: "BatchNormBackward190"
    top: "BatchNormBackward190"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward191"
    type: "ReLU"
    bottom: "BatchNormBackward190"
    top: "BatchNormBackward190"
}
layer {
    name: "ConvNdBackward192"
    type: "Convolution"
    bottom: "BatchNormBackward190"
    top: "ConvNdBackward192"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward193_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward192"
    top: "BatchNormBackward193"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward193_scale"
    type: "Scale"
    bottom: "BatchNormBackward193"
    top: "BatchNormBackward193"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward194"
    type: "ReLU"
    bottom: "BatchNormBackward193"
    top: "BatchNormBackward193"
}
layer {
    name: "ConvNdBackward196"
    type: "Convolution"
    bottom: "ConcatBackward181"
    top: "ConvNdBackward196"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward197_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward196"
    top: "BatchNormBackward197"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward197_scale"
    type: "Scale"
    bottom: "BatchNormBackward197"
    top: "BatchNormBackward197"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward198"
    type: "ReLU"
    bottom: "BatchNormBackward197"
    top: "BatchNormBackward197"
}
layer {
    name: "ConvNdBackward199"
    type: "Convolution"
    bottom: "BatchNormBackward197"
    top: "ConvNdBackward199"
    convolution_param {
        num_output: 160
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward200_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward199"
    top: "BatchNormBackward200"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward200_scale"
    type: "Scale"
    bottom: "BatchNormBackward200"
    top: "BatchNormBackward200"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward201"
    type: "ReLU"
    bottom: "BatchNormBackward200"
    top: "BatchNormBackward200"
}
layer {
    name: "ConvNdBackward202"
    type: "Convolution"
    bottom: "BatchNormBackward200"
    top: "ConvNdBackward202"
    convolution_param {
        num_output: 160
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward203_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward202"
    top: "BatchNormBackward203"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward203_scale"
    type: "Scale"
    bottom: "BatchNormBackward203"
    top: "BatchNormBackward203"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward204"
    type: "ReLU"
    bottom: "BatchNormBackward203"
    top: "BatchNormBackward203"
}
layer {
    name: "ConvNdBackward205"
    type: "Convolution"
    bottom: "BatchNormBackward203"
    top: "ConvNdBackward205"
    convolution_param {
        num_output: 160
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward206_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward205"
    top: "BatchNormBackward206"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward206_scale"
    type: "Scale"
    bottom: "BatchNormBackward206"
    top: "BatchNormBackward206"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward207"
    type: "ReLU"
    bottom: "BatchNormBackward206"
    top: "BatchNormBackward206"
}
layer {
    name: "ConvNdBackward208"
    type: "Convolution"
    bottom: "BatchNormBackward206"
    top: "ConvNdBackward208"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward209_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward208"
    top: "BatchNormBackward209"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward209_scale"
    type: "Scale"
    bottom: "BatchNormBackward209"
    top: "BatchNormBackward209"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward210"
    type: "ReLU"
    bottom: "BatchNormBackward209"
    top: "BatchNormBackward209"
}
layer {
    name: "AvgPool2dBackward212"
    type: "Pooling"
    bottom: "ConcatBackward181"
    top: "AvgPool2dBackward212"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward213"
    type: "Convolution"
    bottom: "AvgPool2dBackward212"
    top: "ConvNdBackward213"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward214_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward213"
    top: "BatchNormBackward214"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward214_scale"
    type: "Scale"
    bottom: "BatchNormBackward214"
    top: "BatchNormBackward214"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward215"
    type: "ReLU"
    bottom: "BatchNormBackward214"
    top: "BatchNormBackward214"
}
layer {
    name: "ConcatBackward216"
    type: "Concat"
    bottom: "BatchNormBackward183"
    bottom: "BatchNormBackward193"
    bottom: "BatchNormBackward209"
    bottom: "BatchNormBackward214"
    top: "ConcatBackward216"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward217"
    type: "Convolution"
    bottom: "ConcatBackward216"
    top: "ConvNdBackward217"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward218_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward217"
    top: "BatchNormBackward218"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward218_scale"
    type: "Scale"
    bottom: "BatchNormBackward218"
    top: "BatchNormBackward218"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward219"
    type: "ReLU"
    bottom: "BatchNormBackward218"
    top: "BatchNormBackward218"
}
layer {
    name: "ConvNdBackward221"
    type: "Convolution"
    bottom: "ConcatBackward216"
    top: "ConvNdBackward221"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward222_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward221"
    top: "BatchNormBackward222"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward222_scale"
    type: "Scale"
    bottom: "BatchNormBackward222"
    top: "BatchNormBackward222"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward223"
    type: "ReLU"
    bottom: "BatchNormBackward222"
    top: "BatchNormBackward222"
}
layer {
    name: "ConvNdBackward224"
    type: "Convolution"
    bottom: "BatchNormBackward222"
    top: "ConvNdBackward224"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward225_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward224"
    top: "BatchNormBackward225"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward225_scale"
    type: "Scale"
    bottom: "BatchNormBackward225"
    top: "BatchNormBackward225"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward226"
    type: "ReLU"
    bottom: "BatchNormBackward225"
    top: "BatchNormBackward225"
}
layer {
    name: "ConvNdBackward227"
    type: "Convolution"
    bottom: "BatchNormBackward225"
    top: "ConvNdBackward227"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward228_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward227"
    top: "BatchNormBackward228"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward228_scale"
    type: "Scale"
    bottom: "BatchNormBackward228"
    top: "BatchNormBackward228"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward229"
    type: "ReLU"
    bottom: "BatchNormBackward228"
    top: "BatchNormBackward228"
}
layer {
    name: "ConvNdBackward231"
    type: "Convolution"
    bottom: "ConcatBackward216"
    top: "ConvNdBackward231"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward232_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward231"
    top: "BatchNormBackward232"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward232_scale"
    type: "Scale"
    bottom: "BatchNormBackward232"
    top: "BatchNormBackward232"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward233"
    type: "ReLU"
    bottom: "BatchNormBackward232"
    top: "BatchNormBackward232"
}
layer {
    name: "ConvNdBackward234"
    type: "Convolution"
    bottom: "BatchNormBackward232"
    top: "ConvNdBackward234"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward235_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward234"
    top: "BatchNormBackward235"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward235_scale"
    type: "Scale"
    bottom: "BatchNormBackward235"
    top: "BatchNormBackward235"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward236"
    type: "ReLU"
    bottom: "BatchNormBackward235"
    top: "BatchNormBackward235"
}
layer {
    name: "ConvNdBackward237"
    type: "Convolution"
    bottom: "BatchNormBackward235"
    top: "ConvNdBackward237"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward238_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward237"
    top: "BatchNormBackward238"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward238_scale"
    type: "Scale"
    bottom: "BatchNormBackward238"
    top: "BatchNormBackward238"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward239"
    type: "ReLU"
    bottom: "BatchNormBackward238"
    top: "BatchNormBackward238"
}
layer {
    name: "ConvNdBackward240"
    type: "Convolution"
    bottom: "BatchNormBackward238"
    top: "ConvNdBackward240"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward241_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward240"
    top: "BatchNormBackward241"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward241_scale"
    type: "Scale"
    bottom: "BatchNormBackward241"
    top: "BatchNormBackward241"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward242"
    type: "ReLU"
    bottom: "BatchNormBackward241"
    top: "BatchNormBackward241"
}
layer {
    name: "ConvNdBackward243"
    type: "Convolution"
    bottom: "BatchNormBackward241"
    top: "ConvNdBackward243"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward244_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward243"
    top: "BatchNormBackward244"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward244_scale"
    type: "Scale"
    bottom: "BatchNormBackward244"
    top: "BatchNormBackward244"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward245"
    type: "ReLU"
    bottom: "BatchNormBackward244"
    top: "BatchNormBackward244"
}
layer {
    name: "AvgPool2dBackward247"
    type: "Pooling"
    bottom: "ConcatBackward216"
    top: "AvgPool2dBackward247"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward248"
    type: "Convolution"
    bottom: "AvgPool2dBackward247"
    top: "ConvNdBackward248"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward249_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward248"
    top: "BatchNormBackward249"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward249_scale"
    type: "Scale"
    bottom: "BatchNormBackward249"
    top: "BatchNormBackward249"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward250"
    type: "ReLU"
    bottom: "BatchNormBackward249"
    top: "BatchNormBackward249"
}
layer {
    name: "ConcatBackward251"
    type: "Concat"
    bottom: "BatchNormBackward218"
    bottom: "BatchNormBackward228"
    bottom: "BatchNormBackward244"
    bottom: "BatchNormBackward249"
    top: "ConcatBackward251"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward252"
    type: "Convolution"
    bottom: "ConcatBackward251"
    top: "ConvNdBackward252"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward253_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward252"
    top: "BatchNormBackward253"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward253_scale"
    type: "Scale"
    bottom: "BatchNormBackward253"
    top: "BatchNormBackward253"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward254"
    type: "ReLU"
    bottom: "BatchNormBackward253"
    top: "BatchNormBackward253"
}
layer {
    name: "ConvNdBackward255"
    type: "Convolution"
    bottom: "BatchNormBackward253"
    top: "ConvNdBackward255"
    convolution_param {
        num_output: 320
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 2
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward256_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward255"
    top: "BatchNormBackward256"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward256_scale"
    type: "Scale"
    bottom: "BatchNormBackward256"
    top: "BatchNormBackward256"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward257"
    type: "ReLU"
    bottom: "BatchNormBackward256"
    top: "BatchNormBackward256"
}
layer {
    name: "ConvNdBackward259"
    type: "Convolution"
    bottom: "ConcatBackward251"
    top: "ConvNdBackward259"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward260_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward259"
    top: "BatchNormBackward260"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward260_scale"
    type: "Scale"
    bottom: "BatchNormBackward260"
    top: "BatchNormBackward260"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward261"
    type: "ReLU"
    bottom: "BatchNormBackward260"
    top: "BatchNormBackward260"
}
layer {
    name: "ConvNdBackward262"
    type: "Convolution"
    bottom: "BatchNormBackward260"
    top: "ConvNdBackward262"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 3
        kernel_h: 1
        kernel_w: 7
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward263_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward262"
    top: "BatchNormBackward263"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward263_scale"
    type: "Scale"
    bottom: "BatchNormBackward263"
    top: "BatchNormBackward263"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward264"
    type: "ReLU"
    bottom: "BatchNormBackward263"
    top: "BatchNormBackward263"
}
layer {
    name: "ConvNdBackward265"
    type: "Convolution"
    bottom: "BatchNormBackward263"
    top: "ConvNdBackward265"
    convolution_param {
        num_output: 192
        pad_h: 3
        pad_w: 0
        kernel_h: 7
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward266_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward265"
    top: "BatchNormBackward266"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward266_scale"
    type: "Scale"
    bottom: "BatchNormBackward266"
    top: "BatchNormBackward266"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward267"
    type: "ReLU"
    bottom: "BatchNormBackward266"
    top: "BatchNormBackward266"
}
layer {
    name: "ConvNdBackward268"
    type: "Convolution"
    bottom: "BatchNormBackward266"
    top: "ConvNdBackward268"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 3
        kernel_w: 3
        stride: 2
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward269_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward268"
    top: "BatchNormBackward269"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward269_scale"
    type: "Scale"
    bottom: "BatchNormBackward269"
    top: "BatchNormBackward269"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward270"
    type: "ReLU"
    bottom: "BatchNormBackward269"
    top: "BatchNormBackward269"
}
layer {
    name: "MaxPool2dBackward272"
    type: "Pooling"
    bottom: "ConcatBackward251"
    top: "MaxPool2dBackward272"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 2
        pad: 0
    }
}
layer {
    name: "ConcatBackward273"
    type: "Concat"
    bottom: "BatchNormBackward256"
    bottom: "BatchNormBackward269"
    bottom: "MaxPool2dBackward272"
    top: "ConcatBackward273"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward274"
    type: "Convolution"
    bottom: "ConcatBackward273"
    top: "ConvNdBackward274"
    convolution_param {
        num_output: 320
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward275_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward274"
    top: "BatchNormBackward275"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward275_scale"
    type: "Scale"
    bottom: "BatchNormBackward275"
    top: "BatchNormBackward275"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward276"
    type: "ReLU"
    bottom: "BatchNormBackward275"
    top: "BatchNormBackward275"
}
layer {
    name: "ConvNdBackward278"
    type: "Convolution"
    bottom: "ConcatBackward273"
    top: "ConvNdBackward278"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward279_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward278"
    top: "BatchNormBackward279"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward279_scale"
    type: "Scale"
    bottom: "BatchNormBackward279"
    top: "BatchNormBackward279"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward280"
    type: "ReLU"
    bottom: "BatchNormBackward279"
    top: "BatchNormBackward279"
}
layer {
    name: "ConvNdBackward281"
    type: "Convolution"
    bottom: "BatchNormBackward279"
    top: "ConvNdBackward281"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 1
        kernel_h: 1
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward282_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward281"
    top: "BatchNormBackward282"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward282_scale"
    type: "Scale"
    bottom: "BatchNormBackward282"
    top: "BatchNormBackward282"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward283"
    type: "ReLU"
    bottom: "BatchNormBackward282"
    top: "BatchNormBackward282"
}
layer {
    name: "ConvNdBackward285"
    type: "Convolution"
    bottom: "BatchNormBackward279"
    top: "ConvNdBackward285"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 0
        kernel_h: 3
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward286_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward285"
    top: "BatchNormBackward286"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward286_scale"
    type: "Scale"
    bottom: "BatchNormBackward286"
    top: "BatchNormBackward286"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward287"
    type: "ReLU"
    bottom: "BatchNormBackward286"
    top: "BatchNormBackward286"
}
layer {
    name: "ConcatBackward288"
    type: "Concat"
    bottom: "BatchNormBackward282"
    bottom: "BatchNormBackward286"
    top: "ConcatBackward288"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward290"
    type: "Convolution"
    bottom: "ConcatBackward273"
    top: "ConvNdBackward290"
    convolution_param {
        num_output: 448
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward291_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward290"
    top: "BatchNormBackward291"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward291_scale"
    type: "Scale"
    bottom: "BatchNormBackward291"
    top: "BatchNormBackward291"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward292"
    type: "ReLU"
    bottom: "BatchNormBackward291"
    top: "BatchNormBackward291"
}
layer {
    name: "ConvNdBackward293"
    type: "Convolution"
    bottom: "BatchNormBackward291"
    top: "ConvNdBackward293"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward294_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward293"
    top: "BatchNormBackward294"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward294_scale"
    type: "Scale"
    bottom: "BatchNormBackward294"
    top: "BatchNormBackward294"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward295"
    type: "ReLU"
    bottom: "BatchNormBackward294"
    top: "BatchNormBackward294"
}
layer {
    name: "ConvNdBackward296"
    type: "Convolution"
    bottom: "BatchNormBackward294"
    top: "ConvNdBackward296"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 1
        kernel_h: 1
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward297_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward296"
    top: "BatchNormBackward297"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward297_scale"
    type: "Scale"
    bottom: "BatchNormBackward297"
    top: "BatchNormBackward297"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward298"
    type: "ReLU"
    bottom: "BatchNormBackward297"
    top: "BatchNormBackward297"
}
layer {
    name: "ConvNdBackward300"
    type: "Convolution"
    bottom: "BatchNormBackward294"
    top: "ConvNdBackward300"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 0
        kernel_h: 3
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward301_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward300"
    top: "BatchNormBackward301"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward301_scale"
    type: "Scale"
    bottom: "BatchNormBackward301"
    top: "BatchNormBackward301"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward302"
    type: "ReLU"
    bottom: "BatchNormBackward301"
    top: "BatchNormBackward301"
}
layer {
    name: "ConcatBackward303"
    type: "Concat"
    bottom: "BatchNormBackward297"
    bottom: "BatchNormBackward301"
    top: "ConcatBackward303"
    concat_param {
        axis: 1
    }
}
layer {
    name: "AvgPool2dBackward305"
    type: "Pooling"
    bottom: "ConcatBackward273"
    top: "AvgPool2dBackward305"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward306"
    type: "Convolution"
    bottom: "AvgPool2dBackward305"
    top: "ConvNdBackward306"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward307_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward306"
    top: "BatchNormBackward307"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward307_scale"
    type: "Scale"
    bottom: "BatchNormBackward307"
    top: "BatchNormBackward307"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward308"
    type: "ReLU"
    bottom: "BatchNormBackward307"
    top: "BatchNormBackward307"
}
layer {
    name: "ConcatBackward309"
    type: "Concat"
    bottom: "BatchNormBackward275"
    bottom: "ConcatBackward288"
    bottom: "ConcatBackward303"
    bottom: "BatchNormBackward307"
    top: "ConcatBackward309"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward310"
    type: "Convolution"
    bottom: "ConcatBackward309"
    top: "ConvNdBackward310"
    convolution_param {
        num_output: 320
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward311_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward310"
    top: "BatchNormBackward311"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward311_scale"
    type: "Scale"
    bottom: "BatchNormBackward311"
    top: "BatchNormBackward311"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward312"
    type: "ReLU"
    bottom: "BatchNormBackward311"
    top: "BatchNormBackward311"
}
layer {
    name: "ConvNdBackward314"
    type: "Convolution"
    bottom: "ConcatBackward309"
    top: "ConvNdBackward314"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward315_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward314"
    top: "BatchNormBackward315"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward315_scale"
    type: "Scale"
    bottom: "BatchNormBackward315"
    top: "BatchNormBackward315"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward316"
    type: "ReLU"
    bottom: "BatchNormBackward315"
    top: "BatchNormBackward315"
}
layer {
    name: "ConvNdBackward317"
    type: "Convolution"
    bottom: "BatchNormBackward315"
    top: "ConvNdBackward317"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 1
        kernel_h: 1
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward318_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward317"
    top: "BatchNormBackward318"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward318_scale"
    type: "Scale"
    bottom: "BatchNormBackward318"
    top: "BatchNormBackward318"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward319"
    type: "ReLU"
    bottom: "BatchNormBackward318"
    top: "BatchNormBackward318"
}
layer {
    name: "ConvNdBackward321"
    type: "Convolution"
    bottom: "BatchNormBackward315"
    top: "ConvNdBackward321"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 0
        kernel_h: 3
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward322_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward321"
    top: "BatchNormBackward322"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward322_scale"
    type: "Scale"
    bottom: "BatchNormBackward322"
    top: "BatchNormBackward322"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward323"
    type: "ReLU"
    bottom: "BatchNormBackward322"
    top: "BatchNormBackward322"
}
layer {
    name: "ConcatBackward324"
    type: "Concat"
    bottom: "BatchNormBackward318"
    bottom: "BatchNormBackward322"
    top: "ConcatBackward324"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward326"
    type: "Convolution"
    bottom: "ConcatBackward309"
    top: "ConvNdBackward326"
    convolution_param {
        num_output: 448
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward327_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward326"
    top: "BatchNormBackward327"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward327_scale"
    type: "Scale"
    bottom: "BatchNormBackward327"
    top: "BatchNormBackward327"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward328"
    type: "ReLU"
    bottom: "BatchNormBackward327"
    top: "BatchNormBackward327"
}
layer {
    name: "ConvNdBackward329"
    type: "Convolution"
    bottom: "BatchNormBackward327"
    top: "ConvNdBackward329"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward330_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward329"
    top: "BatchNormBackward330"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward330_scale"
    type: "Scale"
    bottom: "BatchNormBackward330"
    top: "BatchNormBackward330"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward331"
    type: "ReLU"
    bottom: "BatchNormBackward330"
    top: "BatchNormBackward330"
}
layer {
    name: "ConvNdBackward332"
    type: "Convolution"
    bottom: "BatchNormBackward330"
    top: "ConvNdBackward332"
    convolution_param {
        num_output: 384
        pad_h: 0
        pad_w: 1
        kernel_h: 1
        kernel_w: 3
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward333_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward332"
    top: "BatchNormBackward333"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward333_scale"
    type: "Scale"
    bottom: "BatchNormBackward333"
    top: "BatchNormBackward333"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward334"
    type: "ReLU"
    bottom: "BatchNormBackward333"
    top: "BatchNormBackward333"
}
layer {
    name: "ConvNdBackward336"
    type: "Convolution"
    bottom: "BatchNormBackward330"
    top: "ConvNdBackward336"
    convolution_param {
        num_output: 384
        pad_h: 1
        pad_w: 0
        kernel_h: 3
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward337_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward336"
    top: "BatchNormBackward337"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward337_scale"
    type: "Scale"
    bottom: "BatchNormBackward337"
    top: "BatchNormBackward337"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward338"
    type: "ReLU"
    bottom: "BatchNormBackward337"
    top: "BatchNormBackward337"
}
layer {
    name: "ConcatBackward339"
    type: "Concat"
    bottom: "BatchNormBackward333"
    bottom: "BatchNormBackward337"
    top: "ConcatBackward339"
    concat_param {
        axis: 1
    }
}
layer {
    name: "AvgPool2dBackward341"
    type: "Pooling"
    bottom: "ConcatBackward309"
    top: "AvgPool2dBackward341"
    pooling_param {
        pool: AVE
        kernel_size: 3
        stride: 1
        pad: 1
    }
}
layer {
    name: "ConvNdBackward342"
    type: "Convolution"
    bottom: "AvgPool2dBackward341"
    top: "ConvNdBackward342"
    convolution_param {
        num_output: 192
        pad_h: 0
        pad_w: 0
        kernel_h: 1
        kernel_w: 1
        stride: 1
        bias_term: false
    }
}
layer {
    name: "BatchNormBackward343_bn"
    type: "BatchNorm"
    bottom: "ConvNdBackward342"
    top: "BatchNormBackward343"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    name: "BatchNormBackward343_scale"
    type: "Scale"
    bottom: "BatchNormBackward343"
    top: "BatchNormBackward343"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "ThresholdBackward344"
    type: "ReLU"
    bottom: "BatchNormBackward343"
    top: "BatchNormBackward343"
}
layer {
    name: "ConcatBackward345"
    type: "Concat"
    bottom: "BatchNormBackward311"
    bottom: "ConcatBackward324"
    bottom: "ConcatBackward339"
    bottom: "BatchNormBackward343"
    top: "ConcatBackward345"
    concat_param {
        axis: 1
    }
}
layer {
    name: "AvgPool2dBackward346"
    type: "Pooling"
    bottom: "ConcatBackward345"
    top: "AvgPool2dBackward346"
    pooling_param {
        pool: AVE
        kernel_size: 8
        stride: 8
        pad: 0
    }
}
layer {
    name: "DropoutBackward347"
    type: "Dropout"
    bottom: "AvgPool2dBackward346"
    top: "AvgPool2dBackward346"
    dropout_param {
        dropout_ratio: 0.5
    }
}
layer {
    name: "AddmmBackward348"
    type: "InnerProduct"
    bottom: "AvgPool2dBackward346"
    top: "AddmmBackward348"
    inner_product_param {
        num_output: 1000
    }
}
